{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import utils\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"Simple GCN layer, similar to https://github.com/tkipf/pygcn\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, with_bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if with_bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # self.weight.data.fill_(1)\n",
    "        # if self.bias is not None:\n",
    "        #     self.bias.data.fill_(1)\n",
    "\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        \"\"\" Graph Convolutional Layer forward function\n",
    "        \"\"\"\n",
    "        if input.data.is_sparse:\n",
    "            support = torch.spmm(input, self.weight)\n",
    "        else:\n",
    "            support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    \"\"\" 2 Layer Graph Convolutional Network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nfeat : int\n",
    "        size of input feature dimension\n",
    "    nhid : int\n",
    "        number of hidden units\n",
    "    nclass : int\n",
    "        size of output dimension\n",
    "    dropout : float\n",
    "        dropout rate for GCN\n",
    "    lr : float\n",
    "        learning rate for GCN\n",
    "    weight_decay : float\n",
    "        weight decay coefficient (l2 normalization) for GCN. When `with_relu` is True, `weight_decay` will be set to 0.\n",
    "    with_relu : bool\n",
    "        whether to use relu activation function. If False, GCN will be linearized.\n",
    "    with_bias: bool\n",
    "        whether to include bias term in GCN weights.\n",
    "    device: str\n",
    "        'cpu' or 'cuda'.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "\tWe can first load dataset and then train GCN.\n",
    "\n",
    "    >>> from deeprobust.graph.data import Dataset\n",
    "    >>> from deeprobust.graph.defense import GCN\n",
    "    >>> data = Dataset(root='/tmp/', name='cora')\n",
    "    >>> adj, features, labels = data.adj, data.features, data.labels\n",
    "    >>> idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test\n",
    "    >>> gcn = GCN(nfeat=features.shape[1],\n",
    "              nhid=16,\n",
    "              nclass=labels.max().item() + 1,\n",
    "              dropout=0.5, device='cpu')\n",
    "    >>> gcn = gcn.to('cpu')\n",
    "    >>> gcn.fit(features, adj, labels, idx_train) # train without earlystopping\n",
    "    >>> gcn.fit(features, adj, labels, idx_train, idx_val, patience=30) # train with earlystopping\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout=0.5, lr=0.01, weight_decay=5e-4, with_relu=True, with_bias=True, device=None):\n",
    "\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        assert device is not None, \"Please specify 'device'!\"\n",
    "        self.device = device\n",
    "        self.nfeat = nfeat\n",
    "        self.hidden_sizes = [nhid]\n",
    "        self.nclass = nclass\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid, with_bias=with_bias)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass, with_bias=with_bias)\n",
    "        self.dropout = dropout\n",
    "        self.lr = lr\n",
    "        if not with_relu:\n",
    "            self.weight_decay = 0\n",
    "        else:\n",
    "            self.weight_decay = weight_decay\n",
    "        self.with_relu = with_relu\n",
    "        self.with_bias = with_bias\n",
    "        self.output = None\n",
    "        self.best_model = None\n",
    "        self.best_output = None\n",
    "        self.adj_norm = None\n",
    "        self.features = None\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        if self.with_relu:\n",
    "            x = F.relu(self.gc1(x, adj))\n",
    "        else:\n",
    "            x = self.gc1(x, adj)\n",
    "\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize parameters of GCN.\n",
    "        \"\"\"\n",
    "        self.gc1.reset_parameters()\n",
    "        self.gc2.reset_parameters()\n",
    "\n",
    "    def fit(self, features, adj, labels, idx_train, idx_val=None, train_iters=200, initialize=True, verbose=False, normalize=True, patience=500, **kwargs):\n",
    "        \"\"\"Train the gcn model, when idx_val is not None, pick the best model according to the validation loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        features :\n",
    "            node features\n",
    "        adj :\n",
    "            the adjacency matrix. The format could be torch.tensor or scipy matrix\n",
    "        labels :\n",
    "            node labels\n",
    "        idx_train :\n",
    "            node training indices\n",
    "        idx_val :\n",
    "            node validation indices. If not given (None), GCN training process will not adpot early stopping\n",
    "        train_iters : int\n",
    "            number of training epochs\n",
    "        initialize : bool\n",
    "            whether to initialize parameters before training\n",
    "        verbose : bool\n",
    "            whether to show verbose logs\n",
    "        normalize : bool\n",
    "            whether to normalize the input adjacency matrix.\n",
    "        patience : int\n",
    "            patience for early stopping, only valid when `idx_val` is given\n",
    "        \"\"\"\n",
    "\n",
    "        self.device = self.gc1.weight.device\n",
    "        if initialize:\n",
    "            self.initialize()\n",
    "\n",
    "        if type(adj) is not torch.Tensor:\n",
    "            features, adj, labels = utils.to_tensor(features, adj, labels, device=self.device)\n",
    "        else:\n",
    "            features = features.to(self.device)\n",
    "            adj = adj.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "        if normalize:\n",
    "            if utils.is_sparse_tensor(adj):\n",
    "                adj_norm = utils.normalize_adj_tensor(adj, sparse=True)\n",
    "            else:\n",
    "                adj_norm = utils.normalize_adj_tensor(adj)\n",
    "        else:\n",
    "            adj_norm = adj\n",
    "\n",
    "        self.adj_norm = adj_norm\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "        if idx_val is None:\n",
    "            self._train_without_val(labels, idx_train, train_iters, verbose)\n",
    "        else:\n",
    "            if patience < train_iters:\n",
    "                self._train_with_early_stopping(labels, idx_train, idx_val, train_iters, patience, verbose)\n",
    "            else:\n",
    "                self._train_with_val(labels, idx_train, idx_val, train_iters, verbose)\n",
    "\n",
    "    def _train_without_val(self, labels, idx_train, train_iters, verbose):\n",
    "        self.train()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        for i in range(train_iters):\n",
    "            optimizer.zero_grad()\n",
    "            output = self.forward(self.features, self.adj_norm)\n",
    "            loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            if verbose and i % 10 == 0:\n",
    "                print('Epoch {}, training loss: {}'.format(i, loss_train.item()))\n",
    "\n",
    "        self.eval()\n",
    "        output = self.forward(self.features, self.adj_norm)\n",
    "        self.output = output\n",
    "\n",
    "    def _train_with_val(self, labels, idx_train, idx_val, train_iters, verbose):\n",
    "        if verbose:\n",
    "            print('=== training gcn model ===')\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "        best_loss_val = 100\n",
    "        best_acc_val = 0\n",
    "\n",
    "        for i in range(train_iters):\n",
    "            self.train()\n",
    "            optimizer.zero_grad()\n",
    "            output = self.forward(self.features, self.adj_norm)\n",
    "            loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if verbose and i % 10 == 0:\n",
    "                print('Epoch {}, training loss: {}'.format(i, loss_train.item()))\n",
    "\n",
    "            self.eval()\n",
    "            output = self.forward(self.features, self.adj_norm)\n",
    "            loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "            acc_val = utils.accuracy(output[idx_val], labels[idx_val])\n",
    "\n",
    "            if best_loss_val > loss_val:\n",
    "                best_loss_val = loss_val\n",
    "                self.output = output\n",
    "                weights = deepcopy(self.state_dict())\n",
    "\n",
    "            if acc_val > best_acc_val:\n",
    "                best_acc_val = acc_val\n",
    "                self.output = output\n",
    "                weights = deepcopy(self.state_dict())\n",
    "\n",
    "        if verbose:\n",
    "            print('=== picking the best model according to the performance on validation ===')\n",
    "        self.load_state_dict(weights)\n",
    "\n",
    "    def _train_with_early_stopping(self, labels, idx_train, idx_val, train_iters, patience, verbose):\n",
    "        if verbose:\n",
    "            print('=== training gcn model ===')\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "        early_stopping = patience\n",
    "        best_loss_val = 100\n",
    "\n",
    "        for i in range(train_iters):\n",
    "            self.train()\n",
    "            optimizer.zero_grad()\n",
    "            output = self.forward(self.features, self.adj_norm)\n",
    "            loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if verbose and i % 10 == 0:\n",
    "                print('Epoch {}, training loss: {}'.format(i, loss_train.item()))\n",
    "\n",
    "            self.eval()\n",
    "            output = self.forward(self.features, self.adj_norm)\n",
    "\n",
    "            # def eval_class(output, labels):\n",
    "            #     preds = output.max(1)[1].type_as(labels)\n",
    "            #     return f1_score(labels.cpu().numpy(), preds.cpu().numpy(), average='micro') + \\\n",
    "            #         f1_score(labels.cpu().numpy(), preds.cpu().numpy(), average='macro')\n",
    "\n",
    "            # perf_sum = eval_class(output[idx_val], labels[idx_val])\n",
    "            loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "\n",
    "            if best_loss_val > loss_val:\n",
    "                best_loss_val = loss_val\n",
    "                self.output = output\n",
    "                weights = deepcopy(self.state_dict())\n",
    "                patience = early_stopping\n",
    "            else:\n",
    "                patience -= 1\n",
    "            if i > early_stopping and patience <= 0:\n",
    "                break\n",
    "\n",
    "        if verbose:\n",
    "             print('=== early stopping at {0}, loss_val = {1} ==='.format(i, best_loss_val) )\n",
    "        self.load_state_dict(weights)\n",
    "\n",
    "    def test(self, idx_test):\n",
    "        \"\"\"Evaluate GCN performance on test set.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx_test :\n",
    "            node testing indices\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        output = self.predict()\n",
    "        # output = self.output\n",
    "        loss_test = F.nll_loss(output[idx_test], self.labels[idx_test])\n",
    "        acc_test = utils.accuracy(output[idx_test], self.labels[idx_test])\n",
    "        print(\"Test set results:\",\n",
    "              \"loss= {:.4f}\".format(loss_test.item()),\n",
    "              \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "        return acc_test\n",
    "\n",
    "\n",
    "    def predict(self, features=None, adj=None):\n",
    "        \"\"\"By default, the inputs should be unnormalized data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        features :\n",
    "            node features. If `features` and `adj` are not given, this function will use previous stored `features` and `adj` from training to make predictions.\n",
    "        adj :\n",
    "            adjcency matrix. If `features` and `adj` are not given, this function will use previous stored `features` and `adj` from training to make predictions.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.FloatTensor\n",
    "            output (log probabilities) of GCN\n",
    "        \"\"\"\n",
    "\n",
    "        self.eval()\n",
    "        if features is None and adj is None:\n",
    "            return self.forward(self.features, self.adj_norm)\n",
    "        else:\n",
    "            if type(adj) is not torch.Tensor:\n",
    "                features, adj = utils.to_tensor(features, adj, device=self.device)\n",
    "\n",
    "            self.features = features\n",
    "            if utils.is_sparse_tensor(adj):\n",
    "                self.adj_norm = utils.normalize_adj_tensor(adj, sparse=True)\n",
    "            else:\n",
    "                self.adj_norm = utils.normalize_adj_tensor(adj)\n",
    "            return self.forward(self.features, self.adj_norm)\n",
    "\n",
    "\n"
   ]
  }
 ]
}