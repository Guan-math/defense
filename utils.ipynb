{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.sparse as ts\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    \"\"\"Convert label to onehot format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : numpy.array\n",
    "        node labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.array\n",
    "        onehot labels\n",
    "    \"\"\"\n",
    "    eye = np.eye(labels.max() + 1)\n",
    "    onehot_mx = eye[labels]\n",
    "    return onehot_mx\n",
    "\n",
    "def tensor2onehot(labels):\n",
    "    \"\"\"Convert label tensor to label onehot tensor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : torch.LongTensor\n",
    "        node labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.LongTensor\n",
    "        onehot labels tensor\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    eye = torch.eye(labels.max() + 1)\n",
    "    onehot_mx = eye[labels]\n",
    "    return onehot_mx.to(labels.device)\n",
    "\n",
    "def preprocess(adj, features, labels, preprocess_adj=False, preprocess_feature=False, sparse=False, device='cpu'):\n",
    "    \"\"\"Convert adj, features, labels from array or sparse matrix to\n",
    "    torch Tensor, and normalize the input data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adj : scipy.sparse.csr_matrix\n",
    "        the adjacency matrix.\n",
    "    features : scipy.sparse.csr_matrix\n",
    "        node features\n",
    "    labels : numpy.array\n",
    "        node labels\n",
    "    preprocess_adj : bool\n",
    "        whether to normalize the adjacency matrix\n",
    "    preprocess_feature : bool\n",
    "        whether to normalize the feature matrix\n",
    "    sparse : bool\n",
    "       whether to return sparse tensor\n",
    "    device : str\n",
    "        'cpu' or 'cuda'\n",
    "    \"\"\"\n",
    "\n",
    "    if preprocess_adj:\n",
    "        adj_norm = normalize_adj(adj)\n",
    "\n",
    "    if preprocess_feature:\n",
    "        features = normalize_feature(features)\n",
    "\n",
    "    labels = torch.LongTensor(labels)\n",
    "    if sparse:\n",
    "        adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "        features = sparse_mx_to_torch_sparse_tensor(features)\n",
    "    else:\n",
    "        features = torch.FloatTensor(np.array(features.todense()))\n",
    "        adj = torch.FloatTensor(adj.todense())\n",
    "    return adj.to(device), features.to(device), labels.to(device)\n",
    "\n",
    "def to_tensor(adj, features, labels=None, device='cpu'):\n",
    "    \"\"\"Convert adj, features, labels from array or sparse matrix to\n",
    "    torch Tensor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adj : scipy.sparse.csr_matrix\n",
    "        the adjacency matrix.\n",
    "    features : scipy.sparse.csr_matrix\n",
    "        node features\n",
    "    labels : numpy.array\n",
    "        node labels\n",
    "    device : str\n",
    "        'cpu' or 'cuda'\n",
    "    \"\"\"\n",
    "    if sp.issparse(adj):\n",
    "        adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "    else:\n",
    "        adj = torch.FloatTensor(adj)\n",
    "    if sp.issparse(features):\n",
    "        features = sparse_mx_to_torch_sparse_tensor(features)\n",
    "    else:\n",
    "        features = torch.FloatTensor(np.array(features))\n",
    "\n",
    "    if labels is None:\n",
    "        return adj.to(device), features.to(device)\n",
    "    else:\n",
    "        labels = torch.LongTensor(labels)\n",
    "        return adj.to(device), features.to(device), labels.to(device)\n",
    "\n",
    "def normalize_feature(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mx : scipy.sparse.csr_matrix\n",
    "        matrix to be normalized\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scipy.sprase.lil_matrix\n",
    "        normalized matrix\n",
    "    \"\"\"\n",
    "    if type(mx) is not sp.lil.lil_matrix:\n",
    "        mx = mx.tolil()\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def normalize_adj(mx):\n",
    "    \"\"\"Normalize sparse adjacency matrix,\n",
    "    A' = (D + I)^-1/2 * ( A + I ) * (D + I)^-1/2\n",
    "    Row-normalize sparse matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mx : scipy.sparse.csr_matrix\n",
    "        matrix to be normalized\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scipy.sprase.lil_matrix\n",
    "        normalized matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: maybe using coo format would be better?\n",
    "    if type(mx) is not sp.lil.lil_matrix:\n",
    "        mx = mx.tolil()\n",
    "    if mx[0, 0] == 0 :\n",
    "        mx = mx + sp.eye(mx.shape[0])\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1/2).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    mx = mx.dot(r_mat_inv)\n",
    "    return mx\n",
    "\n",
    "def normalize_sparse_tensor(adj, fill_value=1):\n",
    "    \"\"\"Normalize sparse tensor. Need to import torch_scatter\n",
    "    \"\"\"\n",
    "    edge_index = adj._indices()\n",
    "    edge_weight = adj._values()\n",
    "    num_nodes= adj.size(0)\n",
    "    edge_index, edge_weight = add_self_loops(\n",
    "\tedge_index, edge_weight, fill_value, num_nodes)\n",
    "\n",
    "    row, col = edge_index\n",
    "    from torch_scatter import scatter_add\n",
    "    deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "    values = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    shape = adj.shape\n",
    "    return torch.sparse.FloatTensor(edge_index, values, shape)\n",
    "\n",
    "def add_self_loops(edge_index, edge_weight=None, fill_value=1, num_nodes=None):\n",
    "    # num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    loop_index = torch.arange(0, num_nodes, dtype=torch.long,\n",
    "                              device=edge_index.device)\n",
    "    loop_index = loop_index.unsqueeze(0).repeat(2, 1)\n",
    "\n",
    "    if edge_weight is not None:\n",
    "        assert edge_weight.numel() == edge_index.size(1)\n",
    "        loop_weight = edge_weight.new_full((num_nodes, ), fill_value)\n",
    "        edge_weight = torch.cat([edge_weight, loop_weight], dim=0)\n",
    "\n",
    "    edge_index = torch.cat([edge_index, loop_index], dim=1)\n",
    "\n",
    "    return edge_index, edge_weight\n",
    "\n",
    "def normalize_adj_tensor(adj, sparse=False):\n",
    "    \"\"\"Normalize adjacency tensor matrix.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if adj.is_cuda else \"cpu\")\n",
    "    if sparse:\n",
    "        # warnings.warn('If you find the training process is too slow, you can uncomment line 207 in deeprobust/graph/utils.py. Note that you need to install torch_sparse')\n",
    "        # TODO if this is too slow, uncomment the following code,\n",
    "        # but you need to install torch_scatter\n",
    "        # return normalize_sparse_tensor(adj)\n",
    "        adj = to_scipy(adj)\n",
    "        mx = normalize_adj(adj)\n",
    "        return sparse_mx_to_torch_sparse_tensor(mx).to(device)\n",
    "    else:\n",
    "        mx = adj + torch.eye(adj.shape[0]).to(device)\n",
    "        rowsum = mx.sum(1)\n",
    "        r_inv = rowsum.pow(-1/2).flatten()\n",
    "        r_inv[torch.isinf(r_inv)] = 0.\n",
    "        r_mat_inv = torch.diag(r_inv)\n",
    "        mx = r_mat_inv @ mx\n",
    "        mx = mx @ r_mat_inv\n",
    "    return mx\n",
    "\n",
    "def degree_normalize_adj(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    mx = mx.tolil()\n",
    "    if mx[0, 0] == 0 :\n",
    "        mx = mx + sp.eye(mx.shape[0])\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    # mx = mx.dot(r_mat_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def degree_normalize_sparse_tensor(adj, fill_value=1):\n",
    "    \"\"\"degree_normalize_sparse_tensor.\n",
    "    \"\"\"\n",
    "    edge_index = adj._indices()\n",
    "    edge_weight = adj._values()\n",
    "    num_nodes= adj.size(0)\n",
    "\n",
    "    edge_index, edge_weight = add_self_loops(\n",
    "\tedge_index, edge_weight, fill_value, num_nodes)\n",
    "\n",
    "    row, col = edge_index\n",
    "    from torch_scatter import scatter_add\n",
    "    deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "    deg_inv_sqrt = deg.pow(-1)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "    values = deg_inv_sqrt[row] * edge_weight\n",
    "    shape = adj.shape\n",
    "    return torch.sparse.FloatTensor(edge_index, values, shape)\n",
    "\n",
    "def degree_normalize_adj_tensor(adj, sparse=True):\n",
    "    \"\"\"degree_normalize_adj_tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if adj.is_cuda else \"cpu\")\n",
    "    if sparse:\n",
    "        # return  degree_normalize_sparse_tensor(adj)\n",
    "        adj = to_scipy(adj)\n",
    "        mx = degree_normalize_adj(adj)\n",
    "        return sparse_mx_to_torch_sparse_tensor(mx).to(device)\n",
    "    else:\n",
    "        mx = adj + torch.eye(adj.shape[0]).to(device)\n",
    "        rowsum = mx.sum(1)\n",
    "        r_inv = rowsum.pow(-1).flatten()\n",
    "        r_inv[torch.isinf(r_inv)] = 0.\n",
    "        r_mat_inv = torch.diag(r_inv)\n",
    "        mx = r_mat_inv @ mx\n",
    "    return mx\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    \"\"\"Return accuracy of output compared to labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output : torch.Tensor\n",
    "        output from model\n",
    "    labels : torch.Tensor or numpy.array\n",
    "        node labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        accuracy\n",
    "    \"\"\"\n",
    "    if not hasattr(labels, '__len__'):\n",
    "        labels = [labels]\n",
    "    if type(labels) is not torch.Tensor:\n",
    "        labels = torch.LongTensor(labels)\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def loss_acc(output, labels, targets, avg_loss=True):\n",
    "    if type(labels) is not torch.Tensor:\n",
    "        labels = torch.LongTensor(labels)\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()[targets]\n",
    "    loss = F.nll_loss(output[targets], labels[targets], reduction='mean' if avg_loss else 'none')\n",
    "\n",
    "    if avg_loss:\n",
    "        return loss, correct.sum() / len(targets)\n",
    "    return loss, correct\n",
    "    # correct = correct.sum()\n",
    "    # return loss, correct / len(labels)\n",
    "\n",
    "def classification_margin(output, true_label):\n",
    "    \"\"\"Calculate classification margin for outputs.\n",
    "    `probs_true_label - probs_best_second_class`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output: torch.Tensor\n",
    "        output vector (1 dimension)\n",
    "    true_label: int\n",
    "        true label for this node\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        classification margin for this node\n",
    "    \"\"\"\n",
    "\n",
    "    probs = torch.exp(output)\n",
    "    probs_true_label = probs[true_label].clone()\n",
    "    probs[true_label] = 0\n",
    "    probs_best_second_class = probs[probs.argmax()]\n",
    "    return (probs_true_label - probs_best_second_class).item()\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    sparserow=torch.LongTensor(sparse_mx.row).unsqueeze(1)\n",
    "    sparsecol=torch.LongTensor(sparse_mx.col).unsqueeze(1)\n",
    "    sparseconcat=torch.cat((sparserow, sparsecol),1)\n",
    "    sparsedata=torch.FloatTensor(sparse_mx.data)\n",
    "    return torch.sparse.FloatTensor(sparseconcat.t(),sparsedata,torch.Size(sparse_mx.shape))\n",
    "\n",
    "\t# slower version....\n",
    "    # sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    # indices = torch.from_numpy(\n",
    "    #     np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    # values = torch.from_numpy(sparse_mx.data)\n",
    "    # shape = torch.Size(sparse_mx.shape)\n",
    "    # return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "\n",
    "def to_scipy(tensor):\n",
    "    \"\"\"Convert a dense/sparse tensor to scipy matrix\"\"\"\n",
    "    if is_sparse_tensor(tensor):\n",
    "        values = tensor._values()\n",
    "        indices = tensor._indices()\n",
    "        return sp.csr_matrix((values.cpu().numpy(), indices.cpu().numpy()), shape=tensor.shape)\n",
    "    else:\n",
    "        indices = tensor.nonzero().t()\n",
    "        values = tensor[indices[0], indices[1]]\n",
    "        return sp.csr_matrix((values.cpu().numpy(), indices.cpu().numpy()), shape=tensor.shape)\n",
    "\n",
    "def is_sparse_tensor(tensor):\n",
    "    \"\"\"Check if a tensor is sparse tensor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor : torch.Tensor\n",
    "        given tensor\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        whether a tensor is sparse tensor\n",
    "    \"\"\"\n",
    "    # if hasattr(tensor, 'nnz'):\n",
    "    if tensor.layout == torch.sparse_coo:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_train_val_test(nnodes, val_size=0.1, test_size=0.8, stratify=None, seed=None):\n",
    "    \"\"\"This setting follows nettack/mettack, where we split the nodes\n",
    "    into 10% training, 10% validation and 80% testing data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nnodes : int\n",
    "        number of nodes in total\n",
    "    val_size : float\n",
    "        size of validation set\n",
    "    test_size : float\n",
    "        size of test set\n",
    "    stratify :\n",
    "        data is expected to split in a stratified fashion. So stratify should be labels.\n",
    "    seed : int or None\n",
    "        random seed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    idx_train :\n",
    "        node training indices\n",
    "    idx_val :\n",
    "        node validation indices\n",
    "    idx_test :\n",
    "        node test indices\n",
    "    \"\"\"\n",
    "\n",
    "    assert stratify is not None, 'stratify cannot be None!'\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    idx = np.arange(nnodes)\n",
    "    train_size = 1 - val_size - test_size\n",
    "    # train_test_split function of sklearn\n",
    "    idx_train_and_val, idx_test = train_test_split(idx,\n",
    "                                                   random_state=None,\n",
    "                                                   train_size=train_size + val_size,\n",
    "                                                   test_size=test_size,\n",
    "                                                   stratify=stratify)\n",
    "\n",
    "    if stratify is not None:\n",
    "        stratify = stratify[idx_train_and_val]\n",
    "\n",
    "    idx_train, idx_val = train_test_split(idx_train_and_val,\n",
    "                                          random_state=None,\n",
    "                                          train_size=(train_size / (train_size + val_size)),\n",
    "                                          test_size=(val_size / (train_size + val_size)),\n",
    "                                          stratify=stratify)\n",
    "\n",
    "    return idx_train, idx_val, idx_test\n",
    "\n",
    "def get_train_test(nnodes, test_size=0.8, stratify=None, seed=None):\n",
    "    \"\"\"This function returns training and test set without validation.\n",
    "    It can be used for settings of different label rates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nnodes : int\n",
    "        number of nodes in total\n",
    "    test_size : float\n",
    "        size of test set\n",
    "    stratify :\n",
    "        data is expected to split in a stratified fashion. So stratify should be labels.\n",
    "    seed : int or None\n",
    "        random seed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    idx_train :\n",
    "        node training indices\n",
    "    idx_test :\n",
    "        node test indices\n",
    "    \"\"\"\n",
    "    assert stratify is not None, 'stratify cannot be None!'\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    idx = np.arange(nnodes)\n",
    "    train_size = 1 - test_size\n",
    "    idx_train, idx_test = train_test_split(idx, random_state=None,\n",
    "                                                train_size=train_size,\n",
    "                                                test_size=test_size,\n",
    "                                                stratify=stratify)\n",
    "\n",
    "    return idx_train, idx_test\n",
    "\n",
    "def get_train_val_test_gcn(labels, seed=None):\n",
    "    \"\"\"This setting follows gcn, where we randomly sample 20 instances for each class\n",
    "    as training data, 500 instances as validation data, 1000 instances as test data.\n",
    "    Note here we are not using fixed splits. When random seed changes, the splits\n",
    "    will also change.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : numpy.array\n",
    "        node labels\n",
    "    seed : int or None\n",
    "        random seed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    idx_train :\n",
    "        node training indices\n",
    "    idx_val :\n",
    "        node validation indices\n",
    "    idx_test :\n",
    "        node test indices\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    idx = np.arange(len(labels))\n",
    "    nclass = labels.max() + 1\n",
    "    idx_train = []\n",
    "    idx_unlabeled = []\n",
    "    for i in range(nclass):\n",
    "        labels_i = idx[labels==i]\n",
    "        labels_i = np.random.permutation(labels_i)\n",
    "        idx_train = np.hstack((idx_train, labels_i[: 20])).astype(np.int)\n",
    "        idx_unlabeled = np.hstack((idx_unlabeled, labels_i[20: ])).astype(np.int)\n",
    "\n",
    "    idx_unlabeled = np.random.permutation(idx_unlabeled)\n",
    "    idx_val = idx_unlabeled[: 500]\n",
    "    idx_test = idx_unlabeled[500: 1500]\n",
    "    return idx_train, idx_val, idx_test\n",
    "\n",
    "def get_train_test_labelrate(labels, label_rate):\n",
    "    \"\"\"Get train test according to given label rate.\n",
    "    \"\"\"\n",
    "    nclass = labels.max() + 1\n",
    "    train_size = int(round(len(labels) * label_rate / nclass))\n",
    "    print(\"=== train_size = %s ===\" % train_size)\n",
    "    idx_train, idx_val, idx_test = get_splits_each_class(labels, train_size=train_size)\n",
    "    return idx_train, idx_test\n",
    "\n",
    "def get_splits_each_class(labels, train_size):\n",
    "    \"\"\"We randomly sample n instances for class, where n = train_size.\n",
    "    \"\"\"\n",
    "    idx = np.arange(len(labels))\n",
    "    nclass = labels.max() + 1\n",
    "    idx_train = []\n",
    "    idx_val = []\n",
    "    idx_test = []\n",
    "    for i in range(nclass):\n",
    "        labels_i = idx[labels==i]\n",
    "        labels_i = np.random.permutation(labels_i)\n",
    "        idx_train = np.hstack((idx_train, labels_i[: train_size])).astype(np.int)\n",
    "        idx_val = np.hstack((idx_val, labels_i[train_size: 2*train_size])).astype(np.int)\n",
    "        idx_test = np.hstack((idx_test, labels_i[2*train_size: ])).astype(np.int)\n",
    "\n",
    "    return np.random.permutation(idx_train), np.random.permutation(idx_val), \\\n",
    "           np.random.permutation(idx_test)\n",
    "\n",
    "\n",
    "def unravel_index(index, array_shape):\n",
    "    rows = index // array_shape[1]\n",
    "    cols = index % array_shape[1]\n",
    "    return rows, cols\n",
    "\n",
    "\n",
    "def get_degree_squence(adj):\n",
    "    try:\n",
    "        return adj.sum(0)\n",
    "    except:\n",
    "        return ts.sum(adj, dim=1).to_dense()\n",
    "\n",
    "def likelihood_ratio_filter(node_pairs, modified_adjacency, original_adjacency, d_min, threshold=0.004):\n",
    "    \"\"\"\n",
    "    Filter the input node pairs based on the likelihood ratio test proposed by Zügner et al. 2018, see\n",
    "    https://dl.acm.org/citation.cfm?id=3220078. In essence, for each node pair return 1 if adding/removing the edge\n",
    "    between the two nodes does not violate the unnoticeability constraint, and return 0 otherwise. Assumes unweighted\n",
    "    and undirected graphs.\n",
    "    \"\"\"\n",
    "\n",
    "    N = int(modified_adjacency.shape[0])\n",
    "    # original_degree_sequence = get_degree_squence(original_adjacency)\n",
    "    # current_degree_sequence = get_degree_squence(modified_adjacency)\n",
    "    original_degree_sequence = original_adjacency.sum(0)\n",
    "    current_degree_sequence = modified_adjacency.sum(0)\n",
    "\n",
    "    concat_degree_sequence = torch.cat((current_degree_sequence, original_degree_sequence))\n",
    "\n",
    "    # Compute the log likelihood values of the original, modified, and combined degree sequences.\n",
    "    ll_orig, alpha_orig, n_orig, sum_log_degrees_original = degree_sequence_log_likelihood(original_degree_sequence, d_min)\n",
    "    ll_current, alpha_current, n_current, sum_log_degrees_current = degree_sequence_log_likelihood(current_degree_sequence, d_min)\n",
    "\n",
    "    ll_comb, alpha_comb, n_comb, sum_log_degrees_combined = degree_sequence_log_likelihood(concat_degree_sequence, d_min)\n",
    "\n",
    "    # Compute the log likelihood ratio\n",
    "    current_ratio = -2 * ll_comb + 2 * (ll_orig + ll_current)\n",
    "\n",
    "    # Compute new log likelihood values that would arise if we add/remove the edges corresponding to each node pair.\n",
    "    new_lls, new_alphas, new_ns, new_sum_log_degrees = updated_log_likelihood_for_edge_changes(node_pairs,\n",
    "                                                                                               modified_adjacency, d_min)\n",
    "\n",
    "    # Combination of the original degree distribution with the distributions corresponding to each node pair.\n",
    "    n_combined = n_orig + new_ns\n",
    "    new_sum_log_degrees_combined = sum_log_degrees_original + new_sum_log_degrees\n",
    "    alpha_combined = compute_alpha(n_combined, new_sum_log_degrees_combined, d_min)\n",
    "    new_ll_combined = compute_log_likelihood(n_combined, alpha_combined, new_sum_log_degrees_combined, d_min)\n",
    "    new_ratios = -2 * new_ll_combined + 2 * (new_lls + ll_orig)\n",
    "\n",
    "    # Allowed edges are only those for which the resulting likelihood ratio measure is < than the threshold\n",
    "    allowed_edges = new_ratios < threshold\n",
    "\n",
    "    if allowed_edges.is_cuda:\n",
    "        filtered_edges = node_pairs[allowed_edges.cpu().numpy().astype(np.bool)]\n",
    "    else:\n",
    "        filtered_edges = node_pairs[allowed_edges.numpy().astype(np.bool)]\n",
    "\n",
    "    allowed_mask = torch.zeros(modified_adjacency.shape)\n",
    "    allowed_mask[filtered_edges.T] = 1\n",
    "    allowed_mask += allowed_mask.t()\n",
    "    return allowed_mask, current_ratio\n",
    "\n",
    "\n",
    "def degree_sequence_log_likelihood(degree_sequence, d_min):\n",
    "    \"\"\"\n",
    "    Compute the (maximum) log likelihood of the Powerlaw distribution fit on a degree distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine which degrees are to be considered, i.e. >= d_min.\n",
    "    D_G = degree_sequence[(degree_sequence >= d_min.item())]\n",
    "    try:\n",
    "        sum_log_degrees = torch.log(D_G).sum()\n",
    "    except:\n",
    "        sum_log_degrees = np.log(D_G).sum()\n",
    "    n = len(D_G)\n",
    "\n",
    "    alpha = compute_alpha(n, sum_log_degrees, d_min)\n",
    "    ll = compute_log_likelihood(n, alpha, sum_log_degrees, d_min)\n",
    "    return ll, alpha, n, sum_log_degrees\n",
    "\n",
    "def updated_log_likelihood_for_edge_changes(node_pairs, adjacency_matrix, d_min):\n",
    "    \"\"\" Adopted from https://github.com/danielzuegner/nettack\n",
    "    \"\"\"\n",
    "    # For each node pair find out whether there is an edge or not in the input adjacency matrix.\n",
    "\n",
    "    edge_entries_before = adjacency_matrix[node_pairs.T]\n",
    "    degree_sequence = adjacency_matrix.sum(1)\n",
    "    D_G = degree_sequence[degree_sequence >= d_min.item()]\n",
    "    sum_log_degrees = torch.log(D_G).sum()\n",
    "    n = len(D_G)\n",
    "    deltas = -2 * edge_entries_before + 1\n",
    "    d_edges_before = degree_sequence[node_pairs]\n",
    "\n",
    "    d_edges_after = degree_sequence[node_pairs] + deltas[:, None]\n",
    "\n",
    "    # Sum the log of the degrees after the potential changes which are >= d_min\n",
    "    sum_log_degrees_after, new_n = update_sum_log_degrees(sum_log_degrees, n, d_edges_before, d_edges_after, d_min)\n",
    "    # Updated estimates of the Powerlaw exponents\n",
    "    new_alpha = compute_alpha(new_n, sum_log_degrees_after, d_min)\n",
    "    # Updated log likelihood values for the Powerlaw distributions\n",
    "    new_ll = compute_log_likelihood(new_n, new_alpha, sum_log_degrees_after, d_min)\n",
    "    return new_ll, new_alpha, new_n, sum_log_degrees_after\n",
    "\n",
    "\n",
    "def update_sum_log_degrees(sum_log_degrees_before, n_old, d_old, d_new, d_min):\n",
    "    # Find out whether the degrees before and after the change are above the threshold d_min.\n",
    "    old_in_range = d_old >= d_min\n",
    "    new_in_range = d_new >= d_min\n",
    "    d_old_in_range = d_old * old_in_range.float()\n",
    "    d_new_in_range = d_new * new_in_range.float()\n",
    "\n",
    "    # Update the sum by subtracting the old values and then adding the updated logs of the degrees.\n",
    "    sum_log_degrees_after = sum_log_degrees_before - (torch.log(torch.clamp(d_old_in_range, min=1))).sum(1) \\\n",
    "                                 + (torch.log(torch.clamp(d_new_in_range, min=1))).sum(1)\n",
    "\n",
    "    # Update the number of degrees >= d_min\n",
    "\n",
    "    new_n = n_old - (old_in_range!=0).sum(1) + (new_in_range!=0).sum(1)\n",
    "    new_n = new_n.float()\n",
    "    return sum_log_degrees_after, new_n\n",
    "\n",
    "def compute_alpha(n, sum_log_degrees, d_min):\n",
    "    try:\n",
    "        alpha =  1 + n / (sum_log_degrees - n * torch.log(d_min - 0.5))\n",
    "    except:\n",
    "        alpha =  1 + n / (sum_log_degrees - n * np.log(d_min - 0.5))\n",
    "    return alpha\n",
    "\n",
    "def compute_log_likelihood(n, alpha, sum_log_degrees, d_min):\n",
    "    # Log likelihood under alpha\n",
    "    try:\n",
    "        ll = n * torch.log(alpha) + n * alpha * torch.log(d_min) + (alpha + 1) * sum_log_degrees\n",
    "    except:\n",
    "        ll = n * np.log(alpha) + n * alpha * np.log(d_min) + (alpha + 1) * sum_log_degrees\n",
    "\n",
    "    return ll\n",
    "\n",
    "def ravel_multiple_indices(ixs, shape, reverse=False):\n",
    "    \"\"\"\n",
    "    \"Flattens\" multiple 2D input indices into indices on the flattened matrix, similar to np.ravel_multi_index.\n",
    "    Does the same as ravel_index but for multiple indices at once.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ixs: array of ints shape (n, 2)\n",
    "        The array of n indices that will be flattened.\n",
    "\n",
    "    shape: list or tuple of ints of length 2\n",
    "        The shape of the corresponding matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array of n ints between 0 and shape[0]*shape[1]-1\n",
    "        The indices on the flattened matrix corresponding to the 2D input indices.\n",
    "\n",
    "    \"\"\"\n",
    "    if reverse:\n",
    "        return ixs[:, 1] * shape[1] + ixs[:, 0]\n",
    "\n",
    "    return ixs[:, 0] * shape[1] + ixs[:, 1]\n",
    "\n",
    "def visualize(your_var):\n",
    "    \"\"\"visualize computation graph\"\"\"\n",
    "    from graphviz import Digraph\n",
    "    import torch\n",
    "    from torch.autograd import Variable\n",
    "    from torchviz import make_dot\n",
    "    make_dot(your_var).view()\n",
    "\n",
    "def reshape_mx(mx, shape):\n",
    "    indices = mx.nonzero()\n",
    "    return sp.csr_matrix((mx.data, (indices[0], indices[1])), shape=shape)\n",
    "\n",
    "# def check_path(file_path):\n",
    "#     if not osp.exists(file_path):\n",
    "#         os.system(f'mkdir -p {file_path}')\n",
    "\n"
   ]
  }
 ]
}